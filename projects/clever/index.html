<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>The Curious Layperson: Fine-Grained Image Recognition without Expert Labels</title><meta name=robots content="noindex"><meta name=description content="We propose a novel task that enables fine-grained classification without using expert class information (e.g. bird species) during training. We frame the problem as document retrieval from general image descriptions by leveraging existing textual knowledge bases, such as Wikipedia."><meta name=copyright content="&copy; 2015 to present"><meta http-equiv=cleartype content="on"><meta name=generator content="Hugo 0.85.0"><link rel=stylesheet href=/css/styles.min.234e88cb2abb0cb0194de0deeb50b47fefbd06108a201486e76a549e70d4225a.css integrity=sha256-I06Iyyq7DLAZTeDe61C0f++9BhCKIBSG52pUnnDUIlo=><link rel=icon type=image/png href=/favicon.png></head><body class="flex flex-col min-h-screen"><main class="flex-1 mx-4 md:mx-12 lg:mx-24"><div class="container mx-auto font-robotoslab pb-8 bg-white sm:pb-16 md:pb-20"><div class="text-center mt-10 mx-auto px-4 sm:mt-12 sm:px-6 md:mt-16"><h1 class="font-heebo font-bold tracking-tight text-gray-900 sm:text-2xl md:text-3xl">The Curious Layperson: Fine-Grained Image Recognition without Expert Labels</h1></div><main class="w-10/12 2xl:w-8/12 mx-auto"><div class="text-center w-9/12 mx-auto"><div class="sm:flex sm:flex-grow"><p class="text-x8 sm:text-sm lg:text-base sm:mt-5 border-bottom mx-auto"><a href=https://subhabratachoudhury.com/><span class="border-b border-dashed border-indigo-200">Subhabrata Choudhury</span></a><sup class="ml-px text-x6"></sup></p><p class="text-x8 sm:text-sm lg:text-base sm:mt-5 border-bottom mx-auto"><a href=http://campar.in.tum.de/Main/IroLaina><span class="border-b border-dashed border-indigo-200">Iro Laina</span></a><sup class="ml-px text-x6"></sup></p><p class="text-x8 sm:text-sm lg:text-base sm:mt-5 border-bottom mx-auto"><a href=https://chrirupp.github.io/><span class="border-b border-dashed border-indigo-200">Christian Rupprecht</span></a><sup class="ml-px text-x6"></sup></p><p class="text-x8 sm:text-sm lg:text-base sm:mt-5 border-bottom mx-auto"><a href=https://www.robots.ox.ac.uk/~vedaldi/><span class="border-b border-dashed border-indigo-200">Andrea Vedaldi</span></a><sup class="ml-px text-x6"></sup></p></div><div class="flex items-center"><div class="flex-grow h-16 ..."></div><div class="flex-none w-10 h-auto"><a href=http://www.robots.ox.ac.uk/~vgg/><img class="w-10 h-auto" src=/img/ox.svg alt="VGG, University of Oxford"></a></div><div class="flex-initial h-auto pl-2"><a href=http://www.robots.ox.ac.uk/~vgg/><sup class="text-x7 mr-px"></sup><span class="text-gray-800 border-b border-dashed border-indigo-200 ml-px text-xs sm:text-sm">VGG, University of Oxford</span></a></div><div class="flex-grow h-16 ..."></div></div></div><div class="text-center mt-2"><p class="text-sm font-bold border-bottom sm:mx-auto"><a class="text-ridiculousblue mx-1" href=#paper>paper</a> |
<a class="text-ridiculousblue mx-1" href=#video>video</a> |
<a class="text-ridiculousblue mx-1" href=#code>code</a> |
<a class="text-ridiculousblue mx-1" href=#supp>supplementary</a></p></div><div class="w-10/12 mx-auto pt-4"><div class="mx-auto mt-6"><img class="w-11/12 h-auto w-full object-cover mx-auto" src=/img/p/bmvc21_t.png alt><figcaption class="mt-3 prose-sm text-gray-500 text-justify">We propose a novel task that enables fine-grained classification without using expert class information (e.g. bird species) during training. We frame the problem as document retrieval from general image descriptions by leveraging existing textual knowledge bases, such as Wikipedia.</figcaption></div></div><div class="w-10/12 mx-auto pt-10"><h3 class="text-lg font-bold">Abstract</h3><p class="prose-sm text-justify mt-4 text-gray-800">Most of us are not experts in specific fields, such as ornithology. Nonetheless, we do have general image and language understanding capabilities that we use to match what we see to expert resources. This allows us to expand our knowledge and perform novel tasks without ad-hoc external supervision. On the contrary, machines have a much harder time consulting expert-curated knowledge bases unless trained specifically with that knowledge in mind. Thus, in this paper we consider a new problem: fine-grained image recognition without expert annotations, which we address by leveraging the vast knowledge available in web encyclopedias. First, we learn a model to describe the visual appearance of objects using non-expert image descriptions. We then train a fine- grained textual similarity model that matches image descriptions with documents on a sentence-level basis. We evaluate the method on two datasets and compare with several strong baselines and the state of the art in cross-modal retrieval.</p></div><div class="w-10/12 mx-auto pt-10"><h3 class="text-lg font-bold" id=video>Video</h3><div class="w-11/12 mx-auto mt-6 aspect-w-16 aspect-h-9"><iframe src=6OUxxh46jaA frameborder=0 allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div><div class="w-10/12 mx-auto pt-10"><h3 class="text-lg font-bold">Results</h3><div class="w-11/12 mx-auto mt-6"><img class="h-auto w-full object-cover mx-auto" src=/img/p/bmvc21_r.jpg alt></div></div><div class="w-10/12 mx-auto pt-10"><h3 class="text-lg font-bold" id=paper>Paper</h3><div class="w-11/12 mx-auto mt-6"><div class="flex items-start"><div class="flex-none w-32 h-auto"><a href=/pdfs/choudhury21curious.pdf><img class="w-20 h-auto" src=/img/p/bmvc21_thumb.png alt="paper pdf"></a></div><div class="flex-initial h-auto pl-2"><p class="font-bold text-gray-800">The Curious Layperson: Fine-Grained Image Recognition without Expert Labels</p><p class="prose text-sm text-gray-700">Subhabrata Choudhury,
Iro Laina,
Christian Rupprecht,
Andrea Vedaldi</p><p class="text-x9 text-gray-600 mb-3">BMVC 2021 (Oral)</p><a href=/pdfs/choudhury21curious.pdf><span class="text-gray-800 ml-px text-sm">[ PDF ]</span></a>
<a href="javascript:toggleBibTeX('bibtex')"><span class="text-gray-800 ml-px text-sm">[ BibTeX ]</span></a><div id=bibtex onload="toggleBibTeX('bibtex')" class="hidden m-2 p-2 text-gray-700 rounded-lg border-2 border-dashed border-gray-300"><button onclick="copyToClipboard('bibtex')" class="float-right text-gray-400 active:text-green-500 focus:outline-none"><abbr title="click to copy"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="copy" class="svg-inline--fa z-10 w-4 h-auto" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M384 96V0H272c-26.51.0-48 21.49-48 48v288c0 26.51 21.49 48 48 48H464c26.51.0 48-21.49 48-48V128h-95.1c-18.5.0-32.9-14.4-32.9-32zM416 0v96h96L416 0zM192 352V128H48c-26.51.0-48 21.49-48 48v288c0 26.51 21.49 48 48 48h192c26.51.0 48-21.49 48-48V416h-32c-35.3.0-64-28.7-64-64z"/></svg></abbr></button><div class="whitespace-pre-line p-1 font-mono text-x7">@inproceedings{choudhury21curious,
author = {Choudhury, Subhabrata and Laina, Iro and Rupprecht, Christian and Vedaldi, Andrea},
booktitle = {British Machine Vision Conference}
title = {The Curious Layperson: Fine-Grained Image Recognition without Expert Labels}
volume = {32},
year = {2021}
}</div></div></div></div></div></div><div class="w-10/12 mx-auto pt-10"><h3 class="text-lg font-bold" id=code>Code</h3><div class="w-11/12 mx-auto mt-6"><div class="flex items-center"><div class="flex-none w-32 h-auto"><a href=https://github.com/subhc/clever><img class="w-20 h-auto" src=/img/Octocat.png alt="[ coming soon ]"></a></div><div class="flex-initial h-auto pl-2"><a href=https://github.com/subhc/clever><span class="text-gray-800 ml-px text-sm">[ coming soon ]</span></a></div></div></div></div><div class="w-10/12 mx-auto pt-10"><h3 class="text-lg font-bold" id=supp>Supplementary</h3><div class="w-11/12 mx-auto mt-6"><div class="flex items-center"><div class="flex-none w-32 h-auto"><a href=/pdfs/choudhury21curious_supp.pdf><img class="w-20 h-auto" src=/img/p/bmvc21_supp_thumb.png alt="[ supplementary ]"></a></div><div class="flex-initial h-auto pl-2"><a href=/pdfs/choudhury21curious_supp.pdf><span class="text-gray-800 ml-px text-sm">[ supplementary ]</span></a></div></div></div></div></main></div><script>function toggleBibTeX(b){let a=document.getElementById(b);a.classList.contains("hidden")?a.classList.remove("hidden"):a.classList.add("hidden")}const copyToClipboard=b=>{const a=document.createElement("textarea"),c=document.getElementById(b).getElementsByClassName("whitespace-pre-line")[0].textContent;a.value=c.replace(new RegExp(/\n\s+/g),'\n'),document.body.appendChild(a),a.select(),document.execCommand("copy"),document.body.removeChild(a)}</script></main><script>console.log("Some pages use FontAwesome icons, license is found here: https://fontawesome.com/license")</script></body><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-66564594-1','auto'),ga('send','pageview'))</script></html>