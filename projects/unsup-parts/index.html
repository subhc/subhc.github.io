<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Unsupervised Part Discovery with Contrastive Reconstruction</title><meta name=robots content="noindex"><meta name=description content="We propose an unsupervised method to decompose a images of objects into semantically meaningful parts by building a self-supervised task that encourages the model to learning a semantic decomposition. <b><i>Top:</i></b> In our case, we combine three simple learning principles as “part proxy”: internally-consistent and distinctive appearance, and consistency to transformation (equivariance). <b><i>Bottom:</i></b> Our method works on various fine-grained but visually distinct categories e.g. birds (CUB-2011), human (DeepFashion), sheep (Pascal-Parts) etc."><meta name=copyright content="&copy; 2015 to present"><meta http-equiv=cleartype content="on"><meta name=generator content="Hugo 0.85.0"><link rel=stylesheet href=/css/styles.min.ca092f034d57dcbea3f46d870e808b09718bc662d0d3f3e005e0e16a16ef78a9.css integrity=sha256-ygkvA01X3L6j9G2HDoCLCXGLxmLQ0/PgBeDhahbveKk=><link rel=icon type=image/png href=/favicon.png></head><body class="flex flex-col min-h-screen"><main class="flex-1 mx-4 md:mx-12 lg:mx-24"><div class="container mx-auto font-robotoslab pb-8 bg-white sm:pb-16 md:pb-20"><div class="text-center mt-10 mx-auto px-4 sm:mt-12 sm:px-6 md:mt-16"><h1 class="font-heebo font-bold tracking-tight text-gray-900 sm:text-2xl md:text-3xl">Unsupervised Part Discovery with Contrastive Reconstruction</h1></div><main class="w-10/12 2xl:w-8/12 mx-auto"><div class="text-center w-9/12 mx-auto"><div class="sm:flex sm:flex-grow"><p class="text-x8 sm:text-sm lg:text-base sm:mt-5 border-bottom mx-auto"><a href=https://subhabratachoudhury.com/><span class="border-b border-dashed border-indigo-200">Subhabrata Choudhury</span></a><sup class="ml-px text-x6"></sup></p><p class="text-x8 sm:text-sm lg:text-base sm:mt-5 border-bottom mx-auto"><a href=http://campar.in.tum.de/Main/IroLaina><span class="border-b border-dashed border-indigo-200">Iro Laina</span></a><sup class="ml-px text-x6"></sup></p><p class="text-x8 sm:text-sm lg:text-base sm:mt-5 border-bottom mx-auto"><a href=https://chrirupp.github.io/><span class="border-b border-dashed border-indigo-200">Christian Rupprecht</span></a><sup class="ml-px text-x6"></sup></p><p class="text-x8 sm:text-sm lg:text-base sm:mt-5 border-bottom mx-auto"><a href=https://www.robots.ox.ac.uk/~vedaldi/><span class="border-b border-dashed border-indigo-200">Andrea Vedaldi</span></a><sup class="ml-px text-x6"></sup></p></div><div class="flex items-center"><div class="flex-grow h-16 ..."></div><div class="flex-none w-10 h-auto"><a href=http://www.robots.ox.ac.uk/~vgg/><img class="w-10 h-auto" src=/img/ox.svg alt="VGG, University of Oxford"></a></div><div class="flex-initial h-auto pl-2"><a href=http://www.robots.ox.ac.uk/~vgg/><sup class="text-x7 mr-px"></sup><span class="text-gray-800 border-b border-dashed border-indigo-200 ml-px text-xs sm:text-sm">VGG, University of Oxford</span></a></div><div class="flex-grow h-16 ..."></div></div></div><div class="text-center mt-2"><p class="text-sm font-bold border-bottom sm:mx-auto"><a class="text-ridiculousblue mx-1" href=#paper>paper</a> |
<a class="text-ridiculousblue mx-1" href=#video>video</a> |
<a class="text-ridiculousblue mx-1" href=#code>code</a> |
<a class="text-ridiculousblue mx-1" href=#suppmat>supplementary</a></p></div><div class="w-10/12 mx-auto pt-4"><div class="mx-auto mt-6"><img class="w-11/12 h-auto w-full object-cover mx-auto" src=/img/p/neurips21a-teaser5.jpg alt><figcaption class="mt-3 prose-sm text-gray-500 text-justify">We propose an unsupervised method to decompose a images of objects into semantically meaningful parts by building a self-supervised task that encourages the model to learning a semantic decomposition. <b><i>Top:</i></b> In our case, we combine three simple learning principles as “part proxy”: internally-consistent and distinctive appearance, and consistency to transformation (equivariance). <b><i>Bottom:</i></b> Our method works on various fine-grained but visually distinct categories e.g. birds (CUB-2011), human (DeepFashion), sheep (Pascal-Parts) etc.</figcaption></div></div><div class="w-10/12 mx-auto pt-10"><h3 class="text-lg font-bold">Abstract</h3><p class="prose-sm text-justify mt-4 text-gray-800">The goal of self-supervised visual representation learning is to learn strong, transferable image representations, with the majority of research focusing on object or scene level. On the other hand, representation learning at part level has received significantly less attention. In this paper, we propose an unsupervised approach to object part discovery and segmentation and make three contributions. First, we construct a proxy task through a set of objectives that encourages the model to learn a meaningful decomposition of the image into its parts. Secondly, prior work argues for reconstructing or clustering pre-computed features as a proxy to parts; we show empirically that this alone is unlikely to find meaningful parts; mainly because of their low resolution and the tendency of classification networks to spatially smear out information. We suggest that image reconstruction at the level of pixels can alleviate this problem, acting as a complementary cue. Lastly, we show that the standard evaluation based on keypoint regression does not correlate well with segmentation quality and thus introduce different metrics, NMI and ARI, that better characterize the decomposition of objects into parts. Our method yields semantic parts which are consistent across fine-grained but visually distinct categories, outperforming the state of the art on three benchmark datasets.</p></div><div class="w-10/12 mx-auto pt-10"><h3 class="text-lg font-bold" id=video>Video</h3><div class="w-11/12 mx-auto mt-6 aspect-w-16 aspect-h-9"><iframe src=https://www.youtube-nocookie.com/embed/QH2-TGUlwu4 frameborder=0 allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></div></div><div class="w-10/12 mx-auto pt-10"><h3 class="text-lg font-bold">Results</h3><div class="w-11/12 mx-auto mt-6"><img class="h-auto w-full object-cover mx-auto" src=/img/p/neurips_220_results.jpg alt></div></div><div class="w-10/12 mx-auto pt-10"><h3 class="text-lg font-bold" id=paper>Paper</h3><div class="w-11/12 mx-auto mt-6"><div class="flex items-start"><div class="flex-none w-32 h-auto"><a href><img class="w-20 h-auto" src=/img/file-alt-regular.svg alt="paper pdf"></a></div><div class="flex-initial h-auto pl-2"><p class="font-bold text-gray-800">Unsupervised Part Discovery with Contrastive Reconstruction</p><p class="prose text-sm text-gray-700">Subhabrata Choudhury,
Iro Laina,
Christian Rupprecht,
Andrea Vedaldi</p><p class="text-x9 text-gray-600 mb-3">NeurIPS 2021</p><a href><span class="text-gray-800 ml-px text-sm">[ PDF ]</span></a>
<a href="javascript:toggleBibTeX('bibtex')"><span class="text-gray-800 ml-px text-sm">[ BibTeX ]</span></a><div id=bibtex onload="toggleBibTeX('bibtex')" class="hidden m-2 p-2 text-gray-700 rounded-lg border-2 border-dashed border-gray-300"><button onclick="copyToClipboard('bibtex')" class="float-right text-gray-400 active:text-green-500 focus:outline-none"><abbr title="click to copy"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="copy" class="svg-inline--fa z-10 w-4 h-auto" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M384 96V0H272c-26.51.0-48 21.49-48 48v288c0 26.51 21.49 48 48 48H464c26.51.0 48-21.49 48-48V128h-95.1c-18.5.0-32.9-14.4-32.9-32zM416 0v96h96L416 0zM192 352V128H48c-26.51.0-48 21.49-48 48v288c0 26.51 21.49 48 48 48h192c26.51.0 48-21.49 48-48V416h-32c-35.3.0-64-28.7-64-64z"/></svg></abbr></button><div class="whitespace-pre-line p-1 font-mono text-x7">@inproceedings{choudhury2021unsupervised,
author = {Choudhury, Subhabrata and Laina, Iro and Rupprecht, Christian and Vedaldi, Andrea},
booktitle = {Advances in Neural Information Processing Systems}
title = {Unsupervised Part Discovery with Contrastive Reconstruction}
volume = {34},
year = {2021}
}</div></div></div></div></div></div><div class="w-10/12 mx-auto pt-10"><h3 class="text-lg font-bold" id=code>Code</h3><div class="w-11/12 mx-auto mt-6"><div class="flex items-center"><div class="flex-none w-32 h-auto"><a href=#code><img class="w-20 h-auto" src=/img/Octocat.png alt="[ github link ]"></a></div><div class="flex-initial h-auto pl-2"><a href=#code><span class="text-gray-800 ml-px text-sm">[ github link ]</span></a></div></div></div></div><div class="w-10/12 mx-auto pt-10"><h3 class="text-lg font-bold" id=suppmat>Supplementary</h3><div class="w-11/12 mx-auto mt-6"><div class="flex items-center"><div class="flex-none w-32 h-auto"><a href=#sm><img class="w-20 h-auto" src=/img/file-alt-regular.svg alt="[ supplementary ]"></a></div><div class="flex-initial h-auto pl-2"><a href=#sm><span class="text-gray-800 ml-px text-sm">[ supplementary ]</span></a></div></div></div></div></main></div><script>function toggleBibTeX(b){let a=document.getElementById(b);a.classList.contains("hidden")?a.classList.remove("hidden"):a.classList.add("hidden")}const copyToClipboard=b=>{const a=document.createElement("textarea"),c=document.getElementById(b).getElementsByClassName("whitespace-pre-line")[0].textContent;a.value=c.replace(new RegExp(/\n\s+/g),'\n'),document.body.appendChild(a),a.select(),document.execCommand("copy"),document.body.removeChild(a)}</script></main><script>console.log("Some pages use FontAwesome icons, license is found here: https://fontawesome.com/license")</script></body><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-66564594-1','auto'),ga('send','pageview'))</script></html>